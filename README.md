# AI-Powered Customer Support Ticket Classifier

An intelligent FastAPI service that classifies incoming support tickets by Priority (Urgent, High, Medium, Low) and Department (Tech Support, Billing, Sales). It produces confidence scores, versioned model artifacts, structured logs, and standardized error responses ready for containerized deployment and CI automation.

## Key Features

- Automatic dual-label classification (Priority + Department)
- Confidence scores per prediction
- Versioned model artifacts (`models/v<SEMVER>/`)
- Training metadata + evaluation metrics (`model_metadata.json`, `metrics.json`)
- Structured JSON logging with per-request IDs
- Health & readiness endpoints (Kubernetes-friendly)
- Basic rate limiting & request size limiting
- Standardized error schema (`ErrorResponse`)
- Multi-stage Docker image & GitHub Actions CI pipeline
- Reproducible training with seeded randomness
- Performance baseline harness (Locust) & load test docs

## Tech Stack

| Layer | Technology |
|-------|------------|
| API Framework | FastAPI |
| Language | Python 3.11+ (tested) |
| ML | scikit-learn (TF-IDF + Logistic Regression) |
| Data | pandas / numpy |
| Config | pydantic-settings (`app/config.py`) |
| Logging | Structured JSON + contextvars request ID |
| CI/CD | GitHub Actions (`.github/workflows/ci.yml`) |
| Container | Docker multi-stage build |

## Current Project Structure (Selected)

```
├── app/
│   ├── config.py               # Centralized settings (pydantic BaseSettings)
│   ├── logging_utils.py        # Structured logging + middleware
│   ├── main.py                 # FastAPI app (lifespan, endpoints, rate limiting)
│   └── models/
│       ├── classifier.py       # Training & inference class
│       └── schemas.py          # Pydantic models (Ticket, ErrorResponse, enums)
├── data/
│   └── generate_dataset.py     # Synthetic dataset generator
├── models/
│   └── v1.0.0/                 # Versioned artifacts (created after training)
├── scripts/
│   └── bump_version.py         # Semantic version bump helper
├── tests/                      # Test suite (API + model)
├── train.py                    # Training + metrics persistence
├── load_test.md                # Performance baseline notes
├── locustfile.py               # Locust load generation script
├── Dockerfile                  # Multi-stage build
├── .github/workflows/ci.yml    # CI pipeline
├── .env.example                # Configuration template
├── CHANGELOG.md                # Release notes
├── RELEASE_CHECKLIST.md        # Operational release steps
└── README_PHASE1_UPDATES.md    # Incremental phase documentation
```

## Installation (Local Dev)

```powershell
git clone https://github.com/BoonBoonBoonBoon/AI-Powered-Customer-Support-Ticket-Classifier.git
cd AI-Powered-Customer-Support-Ticket-Classifier
python -m venv .venv
. .venv/Scripts/Activate.ps1
pip install -r requirements.txt
pre-commit install  # optional if contributing
```

Copy `.env.example` to `.env` and adjust if needed.

## Training

Run with autogenerated sample data (1,000 synthetic tickets):

```powershell
python train.py
```

Artifacts created in `models/v1.0.0/` (version comes from `MODEL_VERSION`):

| File | Purpose |
|------|---------|
| `priority_model.joblib` | Serialized priority classifier |
| `department_model.joblib` | Serialized department classifier |
| `priority_vectorizer.joblib` | TF-IDF vectorizer (priority) |
| `department_vectorizer.joblib` | TF-IDF vectorizer (department) |
| `metrics.json` | Validation reports, confusion matrices, summary macro/weighted F1 + accuracy |
| `model_metadata.json` | Data hash, split counts, distributions, macro F1 summary |

Train with custom CSV:

```powershell
python train.py --data path\to\tickets.csv
```

Expected columns: `title,description,priority,department`.

## Running the API

### Local (uvicorn)
```powershell
uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
```

### Docker
```powershell
docker build -t ticket-classifier:latest .
docker run -p 8000:8000 --rm ticket-classifier:latest
```

### Verify
Open: http://localhost:8000/docs

Sample request:
```powershell
curl -X POST http://localhost:8000/classify -H 'Content-Type: application/json' -d '{"title":"Server is down","description":"Main server offline all users impacted"}'
```

## API Endpoints

| Method | Path | Description |
|--------|------|-------------|
| GET | `/` | Basic health (legacy compatibility) |
| GET | `/version` | API + model version & metadata |
| GET | `/health/live` | Liveness probe |
| GET | `/health/ready` | Readiness (model loaded) |
| GET | `/health` | Backward-compatible health (combined) |
| POST | `/classify` | Classify ticket (returns predictions + confidences) |
| GET | `/model/status` | Basic training/load status |

### Classification Response (Example)
```json
{
  "title": "Server is down",
  "description": "Main server offline all users impacted",
  "predicted_priority": "Urgent",
  "predicted_department": "Tech Support",
  "priority_confidence": 0.97,
  "department_confidence": 0.94,
  "customer_email": null
}
```

### Standard Error Response
```json
{
  "detail": "Rate limit exceeded",
  "code": "HTTP_429",
  "request_id": "9f5e2c2d-..."
}
```

## Configuration (`app/config.py`)

Environment variables (see `.env.example`):

| Variable | Default | Description |
|----------|---------|-------------|
| `MODEL_VERSION` | `1.0.0` | Semantic version for artifact directory |
| `MODELS_BASE_DIR` | `models` | Base directory for versioned models |
| `VALIDATION_SPLIT` | `0.2` | Train/validation split ratio |
| `RANDOM_SEED` | `42` | Reproducibility seed |
| `ENABLE_EVAL` | `True` | Whether to compute validation metrics |
| `RATE_LIMIT_REQUESTS` | `100` | Max requests per window (in-memory) |
| `RATE_LIMIT_WINDOW_SEC` | `60` | Window length in seconds |
| `LOG_LEVEL` | `INFO` | Logging verbosity |
| `STRUCTURED_LOGS` | `true` | Enable JSON logs |

## Logging & Observability

Structured JSON logs include request correlation:
```json
{"level":"INFO","logger":"request","message":"request completed","time":1700000000000,
 "request_id":"c6e...","path":"/classify","method":"POST","status_code":200,"latency_ms":12}
```

Added protections:
- Request body size limit: 64 KB
- Basic IP rate limiting (single-process)
- Standardized exception handlers for HTTP + unhandled errors

## Performance Baseline

Use Locust harness:
```powershell
locust -f locustfile.py --host http://localhost:8000
```
Document results in `load_test.md` (latency, RPS, error rate). Establish baseline before optimizations.

## Testing

```powershell
pytest -v
```

Optionally with coverage:
```powershell
pytest --cov=app --cov-report=term-missing
```

## Development Workflow

1. Branch from `dev` (feature/*)
2. Run `pre-commit install` once; commits auto-format & lint (ruff, black, mypy)
3. Add/modify code & tests
4. Ensure `pytest` passes locally
5. Open PR → CI validates (lint, type, security, tests, Docker build)

## Release & Versioning

- Bump `MODEL_VERSION` (or run script `python scripts/bump_version.py <part>`)
- Train model → new `models/vX.Y.Z/` artifacts
- Update `CHANGELOG.md`
- Tag release (`git tag vX.Y.Z && git push --tags`)
- CI can build/publish image (if configured)

## Roadmap (Phase 4 Preview)

- Prometheus metrics endpoint (latency histograms, classification counters)
- Enhanced error taxonomy with stable codes & documentation
- Optional distributed rate limiting (Redis / sliding window)
- Extended inference timing instrumentation

## Contributing

PRs welcome. Please keep changes focused & include tests where practical.

## License

MIT License. See `LICENSE`.

---
For historical phased progress and detailed rationale see `README_PHASE1_UPDATES.md`.