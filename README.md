# AI-Powered Customer Support Ticket Classifier

An intelligent FastAPI service that classifies incoming support tickets by Priority (Urgent, High, Medium, Low) and Department (Tech Support, Billing, Sales). It produces confidence scores, versioned model artifacts, structured logs, and standardized error responses ready for containerized deployment and CI automation.

## Key Features

- Automatic dual-label classification (Priority + Department)
- Confidence scores per prediction
- Versioned model artifacts (`models/v<SEMVER>/`)
- Training metadata + evaluation metrics (`model_metadata.json`, `metrics.json`)
- Structured JSON logging with per-request IDs
- Health & readiness endpoints (Kubernetes-friendly)
- Basic rate limiting & request size limiting
- Standardized error schema (`ErrorResponse`)
- Multi-stage Docker image & GitHub Actions CI pipeline
- Reproducible training with seeded randomness
- Performance baseline harness (Locust) & load test docs

## Tech Stack

| Layer | Technology |
|-------|------------|
| API Framework | FastAPI |
| Language | Python 3.11+ (tested) |
| ML | scikit-learn (TF-IDF + Logistic Regression) |
| Data | pandas / numpy |
| Config | pydantic-settings (`app/config.py`) |
| Logging | Structured JSON + contextvars request ID |
| CI/CD | GitHub Actions (`.github/workflows/ci.yml`) |
| Container | Docker multi-stage build |

## Current Project Structure (Selected)

```
├── app/
│   ├── config.py               # Centralized settings (pydantic BaseSettings)
│   ├── logging_utils.py        # Structured logging + middleware
│   ├── main.py                 # FastAPI app (lifespan, endpoints, rate limiting)
│   └── models/
│       ├── classifier.py       # Training & inference class
│       └── schemas.py          # Pydantic models (Ticket, ErrorResponse, enums)
├── data/
│   └── generate_dataset.py     # Synthetic dataset generator
├── models/
│   └── v1.0.0/                 # Versioned artifacts (created after training)
├── scripts/
│   └── bump_version.py         # Semantic version bump helper
├── tests/                      # Test suite (API + model)
├── train.py                    # Training + metrics persistence
├── load_test.md                # Performance baseline notes
├── locustfile.py               # Locust load generation script
├── Dockerfile                  # Multi-stage build
├── .github/workflows/ci.yml    # CI pipeline
├── .env.example                # Configuration template
├── CHANGELOG.md                # Release notes
├── RELEASE_CHECKLIST.md        # Operational release steps
└── README_PHASE1_UPDATES.md    # Incremental phase documentation
```

## Installation (Local Dev)

```powershell
git clone https://github.com/BoonBoonBoonBoon/AI-Powered-Customer-Support-Ticket-Classifier.git
cd AI-Powered-Customer-Support-Ticket-Classifier
python -m venv .venv
. .venv/Scripts/Activate.ps1
pip install -r requirements.txt
pre-commit install  # optional if contributing
```

Copy `.env.example` to `.env` and adjust if needed.

## Training

Run with autogenerated sample data (1,000 synthetic tickets):

```powershell
python train.py
```

Artifacts created in `models/v1.0.0/` (version comes from `MODEL_VERSION`):

| File | Purpose |
|------|---------|
| `priority_model.joblib` | Serialized priority classifier |
| `department_model.joblib` | Serialized department classifier |
| `priority_vectorizer.joblib` | TF-IDF vectorizer (priority) |
| `department_vectorizer.joblib` | TF-IDF vectorizer (department) |
| `metrics.json` | Validation reports, confusion matrices, summary macro/weighted F1 + accuracy |
| `model_metadata.json` | Data hash, split counts, distributions, macro F1 summary |

Train with custom CSV:

```powershell
python train.py --data path\to\tickets.csv
```

Expected columns: `title,description,priority,department`.

## Running the API

### Local (uvicorn)
```powershell
uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
```

### Docker
```powershell
docker build -t ticket-classifier:latest .
docker run -p 8000:8000 --rm ticket-classifier:latest
```

### Verify
Open: http://localhost:8000/docs

Sample request:
```powershell
curl -X POST http://localhost:8000/classify -H 'Content-Type: application/json' -d '{"title":"Server is down","description":"Main server offline all users impacted"}'
```

## API Endpoints

| Method | Path | Description |
|--------|------|-------------|
| GET | `/` | Basic health (legacy compatibility) |
| GET | `/version` | API + model version & metadata |
| GET | `/health/live` | Liveness probe |
| GET | `/health/ready` | Readiness (model loaded) |
| GET | `/health` | Backward-compatible health (combined) |
| POST | `/classify` | Classify ticket (returns predictions + confidences) |
| GET | `/model/status` | Basic training/load status |

### Classification Response (Example)
```json
{
  "title": "Server is down",
  "description": "Main server offline all users impacted",
  "predicted_priority": "Urgent",
  "predicted_department": "Tech Support",
  "priority_confidence": 0.97,
  "department_confidence": 0.94,
  "customer_email": null
}
```

### Standard Error Response
```json
{
  "detail": "Rate limit exceeded",
  "code": "HTTP_429",
  "request_id": "9f5e2c2d-..."
}
```

## Configuration (`app/config.py`)

Environment variables (see `.env.example`):

| Variable | Default | Description |
|----------|---------|-------------|
| `MODEL_VERSION` | `1.0.0` | Semantic version for artifact directory |
| `MODELS_BASE_DIR` | `models` | Base directory for versioned models |
| `VALIDATION_SPLIT` | `0.2` | Train/validation split ratio |
| `RANDOM_SEED` | `42` | Reproducibility seed |
| `ENABLE_EVAL` | `True` | Whether to compute validation metrics |
| `RATE_LIMIT_REQUESTS` | `100` | Max requests per window (in-memory) |
| `RATE_LIMIT_WINDOW_SEC` | `60` | Window length in seconds |
| `LOG_LEVEL` | `INFO` | Logging verbosity |
| `STRUCTURED_LOGS` | `true` | Enable JSON logs |

## Logging & Observability

Structured JSON logs include request correlation:
```json
{"level":"INFO","logger":"request","message":"request completed","time":1700000000000,
 "request_id":"c6e...","path":"/classify","method":"POST","status_code":200,"latency_ms":12}
```

Added protections:
- Request body size limit: 64 KB
- Basic IP rate limiting (single-process)
- Standardized exception handlers for HTTP + unhandled errors

## Performance Baseline

Use Locust harness:
```powershell
locust -f locustfile.py --host http://localhost:8000
```
Document results in `load_test.md` (latency, RPS, error rate). Establish baseline before optimizations.

## Testing

```powershell
pytest -v
```

Optionally with coverage:
```powershell
pytest --cov=app --cov-report=term-missing
```

## Development Workflow

1. Branch from `dev` (feature/*)
2. Run `pre-commit install` once; commits auto-format & lint (ruff, black, mypy)
3. Add/modify code & tests
4. Ensure `pytest` passes locally
5. Open PR → CI validates (lint, type, security, tests, Docker build)

## Release & Versioning

- Bump `MODEL_VERSION` (or run script `python scripts/bump_version.py <part>`)
- Train model → new `models/vX.Y.Z/` artifacts
- Update `CHANGELOG.md`
- Tag release (`git tag vX.Y.Z && git push --tags`)
- CI can build/publish image (if configured)

## Roadmap (Phase 4 Preview)

- Prometheus metrics endpoint (latency histograms, classification counters)
- Enhanced error taxonomy with stable codes & documentation
- Optional distributed rate limiting (Redis / sliding window)
- Extended inference timing instrumentation

## Deployment: Railway

You can deploy directly to Railway using either the buildpack (Procfile) or Docker.

### Option A: Buildpack (Zero-Config)
Included files:
- `Procfile` → `web: uvicorn app.main:app --host 0.0.0.0 --port ${PORT:-8000}`
- `runtime.txt` → Pins Python version
- Root `main.py` → Fallback export for FastAPI auto-detection

Steps:
1. Create a new Railway project and connect this repository.
2. Select the `dev` or `main` branch.
3. Railway auto-detects Python & installs `requirements.txt`.
4. Startup command comes from `Procfile` (no manual override needed).
5. After deploy, visit `https://<service>/health/ready`.

Environment variables (set in Railway UI as needed):
| Var | Suggested | Reason |
|-----|-----------|-------|
| `LOG_LEVEL` | `info` | Runtime log verbosity |
| `STRUCTURED_LOGS` | `true` | JSON logs for aggregators |
| `RATE_LIMIT_REQUESTS` | `100` | Adjust per expected RPS |
| `RATE_LIMIT_WINDOW_SEC` | `60` | Align with rate policy |

### Option B: Docker Deploy
1. In Railway project settings choose Dockerfile deployment.
2. Railway builds image using multi-stage `Dockerfile`.
3. Exposes port 8000 (Railway maps to `$PORT`).

### Post-Deploy Smoke Test
```bash
curl https://<service-domain>/health/live
curl https://<service-domain>/health/ready
curl -X POST https://<service-domain>/classify \
  -H 'Content-Type: application/json' \
  -d '{"title":"Server down","description":"Main server offline all users impacted"}'
```

If you see `503` for readiness: ensure training artifacts exist in `models/v<version>/` committed to repo (or mount storage & run `train.py`).

### Common Railway Issues
| Symptom | Cause | Fix |
|---------|-------|-----|
| "No start command was found" | Missing Procfile / app export | Already fixed by added `Procfile` & root `main.py` |
| 503 on classify | Models not loaded | Run training locally, commit `models/vX.Y.Z/*.pkl` |
| 429 errors | Rate limit triggered | Increase `RATE_LIMIT_REQUESTS` or widen window |
| 413 errors | Large payload | Reduce body size or adjust middleware (code change) |

## Contributing

PRs welcome. Please keep changes focused & include tests where practical.

## License

MIT License. See `LICENSE`.

---
For historical phased progress and detailed rationale see `README_PHASE1_UPDATES.md`.

## Hardening & Real-World Standards

This project is being incrementally aligned with production-grade expectations. The following controls are implemented or planned:

### Implemented
- Deterministic training: seeded randomness + versioned artifacts (`models/v1.0.0/`).
- Structured logging: JSON logs with `request_id` correlation.
- Runtime safeguards: body size limit (64KB), basic in-memory rate limiting, standardized error schema.
- CI quality gates: lint (ruff), formatting (black), types (mypy), unit tests (pytest), dependency audit (pip-audit advisory).
- Container best practices: multi-stage build, non-root user, minimized base image.
- Dependency locking: `requirements.lock` for reproducible builds.
- Release hygiene: CHANGELOG, release checklist, semantic model versioning.
- Performance baseline: Locust script + documented methodology.

### In Progress / Phase 4 Targets
- Observability metrics: Prometheus `/metrics` endpoint (request counts, latency histograms, inference timing).
- Enhanced error taxonomy: central catalog of error codes and mapping to HTTP statuses.
- Security upgrades: optional Bandit SAST stage (fail on high severity), SBOM generation (CycloneDX or Syft) in CI.
- Rate limiting evolution: pluggable backend (Redis) + sliding window or token bucket algorithm.
- Model monitoring hooks: log distribution drift indicators (placeholder for later integration).
- Build provenance: container image signing (cosign) & attestation (future enhancement).

### Planned CI Enhancements (Proposed Jobs)
| Job | Purpose | Tooling |
|-----|---------|---------|
| `sast` | Static security analysis | Bandit |
| `sbom` | Generate and upload SBOM | cyclonedx-bom or syft |
| `metrics-test` | Validate metrics endpoint shape | curl + jq schema check |
| `perf-smoke` | Quick latency sanity check | k6 or Locust headless |
| `image-sign` | Sign built image | cosign |

### Operational Recommendations
- Configure log aggregation (ELK, Loki, or Cloud Logs) parsing JSON lines.
- Set Kubernetes resource requests/limits informed by Locust baseline.
- Add health/readiness probes using `/health/live` and `/health/ready`.
- Enable horizontal autoscaling once metrics exported (RPS & latency triggers).
- Periodically snapshot `metrics.json` into artifact storage for model performance tracking.

### Security Posture Roadmap
| Layer | Current | Next Step |
|-------|---------|-----------|
| Dependencies | pip-audit advisory | Enforce fail-on-high, add SBOM diff | 
| Code | Type + lint checks | Bandit & secret scanning | 
| Runtime | Non-root, size limit, rate limit | Add WAF / API key auth (if external) | 
| Supply Chain | Multi-stage Docker | Image signing & provenance attestations | 

### Contributing to Hardening
When opening PRs for hardening tasks, prefix titles with `hardening:` or `security:` and include:
1. Rationale / threat or reliability risk addressed.
2. Success criteria & verification steps.
3. Rollback considerations.

---

## Evaluation & Visual Testing

To understand model behavior quickly, two helper scripts are provided under `scripts/`:

| Script | Purpose | Typical Target |
|--------|---------|----------------|
| `scripts/smoke_test.py` | Fast endpoint & single prediction health check (colored output) | Local / CI / Prod URL |
| `scripts/bulk_eval.py` | Batch evaluation over labeled CSV with per-row table & metrics | Local / Staging / Prod shadow |

### 1. Smoke Test
Runs liveness, readiness, version, and one classification.

Local default (assumes `uvicorn` on port 8000):
```powershell
python scripts/smoke_test.py
```

Remote deployment:
```powershell
python scripts/smoke_test.py --base-url https://YOUR-DOMAIN.up.railway.app
```

Show full JSON payloads:
```powershell
python scripts/smoke_test.py --base-url https://YOUR-DOMAIN.up.railway.app --show-json
```

Exit code non‑zero if any step fails (use in CI gates).

### 2. Bulk Evaluation
Evaluates all rows in `data/mock_eval.csv` (add or replace with real samples). Each row must contain `title,description`; optional `priority,department` labels enable accuracy & macro-F1 computation.

Basic run (local):
```powershell
python scripts/bulk_eval.py --base-url http://localhost:8000
```

Against deployment, create a Markdown report:
```powershell
python scripts/bulk_eval.py `
  --base-url https://YOUR-DOMAIN.up.railway.app `
  --csv data/mock_eval.csv `
  --md-report reports/eval_report.md
```

Threshold enforcement (fail build if accuracy drops):
```powershell
python scripts/bulk_eval.py --base-url http://localhost:8000 \
  --fail-threshold-priority-acc 0.85 \
  --fail-threshold-dept-acc 0.95
```

Output includes a colorized table:
```
Title                        | P_true | P_pred | P_conf | MatchP | D_true | D_pred | D_conf | MatchD
-----------------------------+--------+--------+--------+--------+--------+--------+--------+-------
Server outage                | Urgent | Urgent | 0.61   | Y      | Tech Support | Tech Support | 0.80 | Y
Partial degradation          | High   | High   | 0.74   | Y      | Tech Support | Tech Support | 0.88 | Y
...
```

Markdown report (if path supplied) stores metrics + a simple table for sharing in PRs.

### 3. Workflow Suggestion
1. Run `python train.py` (updates artifacts & metrics).
2. Run `python scripts/smoke_test.py` locally – ensure green.
3. Run `python scripts/bulk_eval.py --md-report reports/eval_report.md` – inspect misclassifications.
4. Commit `reports/eval_report.md` if illustrating a model change in a PR.
5. (Future) CI job invokes smoke and bulk eval with thresholds to block regressions.

### 4. Extending Evaluation
- Add additional CSVs (e.g., `data/edge_cases.csv`) and run bulk eval per file.
- Introduce a `--jsonl-output` flag (future) to persist raw predictions for downstream drift tools.
- Feed misclassified samples back into next training cycle for targeted improvement.

### 5. Understanding Confidence
- `priority_confidence` & `department_confidence` are the max softmax probabilities from logistic regression.
- Low confidence (< ~0.55) can be routed to human review; incorporate later as a threshold-based decision rule.
- Track average confidence over time to detect silent model degradation.

---

